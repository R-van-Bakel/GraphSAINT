{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907cdc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from functools import partial\n",
    "from time import time\n",
    "# from scipy.sparse import csr_matrix\n",
    "\n",
    "# https://stackoverflow.com/questions/52299420/scipy-csr-matrix-understand-indptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948bd722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class myGraphSAINT(nn.Module):\n",
    "#     def __init__(self, hidden_sizes):\n",
    "#         super(myGraphSAINT, self).__init__()\n",
    "# #         self.layers = nn.ModuleList([nn.Linear(hidden_sizes[i], hidden_sizes[i+1], bias=False)\n",
    "# #                                      for i in range(len(hidden_sizes)-1)])\n",
    "#         self.weights = [\n",
    "#             nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((hidden_sizes[i], hidden_sizes[i+1])), gain=1/np.sqrt(6.0)))\n",
    "#             for i in range(len(hidden_sizes)-1)\n",
    "#         ]\n",
    "    \n",
    "#     def forward(self, x, A):\n",
    "#         for W in self.weights:\n",
    "#             x = nn.functional.relu(A @ x @ W)\n",
    "#         return x\n",
    "    \n",
    "#     def sampleGraph(self, graph):\n",
    "#         pass\n",
    "    \n",
    "#     def train(self, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad9904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((2, 3)), gain=1.0)))\n",
    "# x = torch.rand((4,3))\n",
    "# A = torch.eye(4)\n",
    "# model = myGraphSAINT([3,10,3])\n",
    "# print(model(x, A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c0d9c6",
   "metadata": {},
   "source": [
    "## Data loading and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dc6dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_train = np.load(\"./ppi/adj_train.npz\")\n",
    "features = np.load(\"./ppi/feats.npy\")\n",
    "\n",
    "# The data for the (symmetric) adjacency matrix\n",
    "data = adj_train[\"data\"]\n",
    "data = data.astype(int)\n",
    "indices = adj_train[\"indices\"]\n",
    "indptr = adj_train[\"indptr\"]\n",
    "shape = adj_train[\"shape\"]\n",
    "\n",
    "# # This is a less memory-efficient method to get the sparse torch adjacency matrix, that also requires SciPy\n",
    "# adj_matrix = csr_matrix((data, indices, indptr), shape=shape).toarray().astype(int)\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "# adj_matrix = torch.from_numpy(adj_matrix).to_sparse()\n",
    "\n",
    "# Change the SciPy csr format to torch format\n",
    "torch_first_indices = []\n",
    "for i in range(len(indptr)-1):\n",
    "    torch_first_indices += [i for ind in indices[indptr[i]:indptr[i+1]]]\n",
    "torch_first_indices = np.asarray(torch_first_indices)\n",
    "torch_indices = np.stack((torch_first_indices, indices))\n",
    "\n",
    "# The given shape for the ppi train set is much larger than the number of actual nodes in the set\n",
    "num_nodes = len(np.unique(torch_indices))\n",
    "shape_small = [num_nodes, num_nodes]\n",
    "\n",
    "# Create the adjacency matrix\n",
    "adj_matrix = torch.sparse_coo_tensor(indices=torch_indices, values=data, size=shape_small, dtype=torch.float)\n",
    "\n",
    "# Calculate the node degrees\n",
    "degree = [0.0 for i in range(shape_small[0])]\n",
    "for i in torch_indices.T:\n",
    "    degree[i[0]] += 1\n",
    "    if i[1] == i[0]:\n",
    "        degree[i[0]] += 1\n",
    "inverse_degree = np.reciprocal(np.asarray(degree))\n",
    "\n",
    "# Calculate the normalized adjacency matrix\n",
    "norm_adj_data = inverse_degree[torch_indices[0]]*data.astype(float)\n",
    "# norm_adj_matrix = torch.sparse_coo_tensor(indices=torch_indices, values=norm_adj_data.astype(float), size=shape_small,\n",
    "#                                           dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d830648",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c549cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_nodes(num_nodes, budget, p_nodes, p_edges, indices, data, features):\n",
    "    # Sample\n",
    "#     t5 = time()\n",
    "    nodes_s = np.unique(np.random.choice(np.arange(num_nodes), size=budget, p=p_nodes, replace=False))\n",
    "#     t6 = time()\n",
    "#     print(f\"Node samlping time: {t6-t5}\")\n",
    "    \n",
    "    # Connect the sampled nodes\n",
    "#     t3 = time()\n",
    "    condition = np.all(np.in1d(indices.flatten(), nodes_s).reshape(indices.shape), axis=0)\n",
    "    edges_s = indices[:,condition]\n",
    "    data_s = data[condition]\n",
    "    edge_indices = np.where(condition)[0]\n",
    "    nodes_s = set(nodes_s)\n",
    "#     edges_s = []\n",
    "#     data_s = []\n",
    "#     edge_indices = []\n",
    "#     for i, edge in enumerate(indices.transpose()):\n",
    "#         if edge[0] in nodes_s and edge[1] in nodes_s:\n",
    "#             edges_s.append(edge)\n",
    "#     #         edges_s.append([orig2sub[edge[0]], orig2sub[edge[1]]])\n",
    "#             data_s.append(data[i])\n",
    "#             edge_indices.append(i)\n",
    "#     edges_s = np.asarray(edges_s).transpose()\n",
    "#     data_s = np.asarray(data_s)\n",
    "#     print(edges_s.dtype)\n",
    "#     print(data_s.dtype)\n",
    "#     t4 = time()\n",
    "#     print(f\"Loop time: {t4-t3}\")\n",
    "\n",
    "    # Remove unconnected nodes (not connected to other nodes or themselves)\n",
    "#     t7 = time()\n",
    "    len_before = len(nodes_s)\n",
    "    nodes_s = nodes_s.intersection(set(np.unique(edges_s)))\n",
    "    len_after = len(nodes_s)\n",
    "    budget -= len_before-len_after\n",
    "#     t8 = time()\n",
    "#     print(f\"Remove time: {t8-t7}\")\n",
    "    # If no nodes are connected, retry sampling\n",
    "    if len(nodes_s) == 0:\n",
    "        return sample_nodes(num_nodes, budget, p_nodes, p_edges, indices, data, features)\n",
    "    \n",
    "    orig2sub = {ind : i for i, ind in enumerate(nodes_s)}\n",
    "    nodes_s_sub = {orig2sub[node] for node in nodes_s}\n",
    "    edges_s_sub = np.vectorize(orig2sub.get)(edges_s)\n",
    "\n",
    "    # Create the adjacency matrix of the sampled graph\n",
    "    adj_matrix_s = adj_matrix = torch.sparse_coo_tensor(indices=edges_s_sub, values=data_s, size=[budget, budget],\n",
    "                                                        dtype=torch.float)\n",
    "    \n",
    "    # Calculate the normalizing constants\n",
    "    p_nodes_s = np.take(p_nodes, list(edges_s[0]))\n",
    "    p_edges_s = np.take(p_edges, list(edge_indices))\n",
    "    alpha = p_edges_s/p_nodes_s\n",
    "    alpha_matrix = torch.sparse_coo_tensor(indices=edges_s_sub, values=alpha, size=[budget, budget],\n",
    "                                           dtype=torch.float)\n",
    "    return adj_matrix_s * alpha_matrix, torch.from_numpy(features[list(nodes_s)]).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7657d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "budget = 6000 # 18\n",
    "\n",
    "# Calculate the sampling probablities\n",
    "p_nodes = [0.0 for i in range(shape_small[0])]\n",
    "for i, ind in enumerate(torch_indices[1]):\n",
    "    p_nodes[ind] += norm_adj_data[i]**2\n",
    "p_nodes = np.asarray(p_nodes)\n",
    "p_nodes = p_nodes/p_nodes.sum()\n",
    "\n",
    "# Get the edge probabilities\n",
    "# For the node sampler, the probability of an edge being sampled, is just to probability of both it's nodes being sampled\n",
    "# Note that for an edge connecting a node to itself, the probability of sampling it is just the probability of sampling the node\n",
    "self_loops = np.where(np.all(torch_indices == torch_indices[0,:], axis = 0)==True)\n",
    "p_edges = np.take(p_nodes, torch_indices)\n",
    "np.put(p_edges[1], self_loops, 1)\n",
    "p_edges = p_edges.prod(0)\n",
    "\n",
    "# p_edge_matrix = torch.sparse_coo_tensor(indices=torch_indices, values=p_edges, size=shape_small, dtype=torch.float64)\n",
    "\n",
    "nodewise_sampler = partial(sample_nodes, num_nodes, budget, p_nodes, p_edges, torch_indices, norm_adj_data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a73b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_weight_matrix(matrix, indices):\n",
    "#     return matrix[sorted(indices)][:, sorted(indices)]\n",
    "\n",
    "# W = torch.rand(shape_small)\n",
    "# W_batch = sample_weight_matrix(W, nodes_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30777e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myGraphSAINT(nn.Module):\n",
    "    def __init__(self, hidden_sizes, lr, sampler):\n",
    "        super(myGraphSAINT, self).__init__()\n",
    "        self.weights = torch.nn.ParameterList(\n",
    "            nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((hidden_sizes[i], hidden_sizes[i+1])), gain=1/np.sqrt(6.0)))\n",
    "            for i in range(len(hidden_sizes)-1)\n",
    "        )\n",
    "        self.sampler = sampler\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "    \n",
    "    def forward(self):\n",
    "#         t1 = time()\n",
    "        A, x = self.sampleGraph()\n",
    "#         t2 = time()\n",
    "#         print(f\"Sampling time: {t2-t1}\")\n",
    "        for W in self.weights:\n",
    "            x = nn.functional.relu(A @ x @ W)\n",
    "        return x\n",
    "    \n",
    "    def sampleGraph(self):\n",
    "        return self.sampler()\n",
    "    \n",
    "    def train_step(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        y = self()\n",
    "        temp_test = torch.ones_like(y)\n",
    "        loss = torch.nn.functional.mse_loss(y, temp_test)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def fit(self, num_iterations):\n",
    "        self.train()\n",
    "        losses = []\n",
    "        for i in range(num_iterations):\n",
    "            losses.append(self.train_step())\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016af24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes_samp = np.unique(np.random.choice(np.arange(num_nodes), size=budget, p=p_nodes, replace=False))\n",
    "# ta = time()\n",
    "# condition = np.all(np.in1d(torch_indices.flatten(), nodes_samp).reshape(torch_indices.shape), axis=0)\n",
    "# torch_indices[:,condition]\n",
    "# print(np.where(condition)[0])\n",
    "# tb = time()\n",
    "# print(tb-ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847b2b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = myGraphSAINT([50, 128], 0.01, nodewise_sampler)\n",
    "t_start = time()\n",
    "losses = model.fit(10)\n",
    "t_end = time()\n",
    "print(t_end-t_start)\n",
    "print(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
