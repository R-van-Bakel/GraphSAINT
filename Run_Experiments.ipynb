{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907cdc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from functools import partial\n",
    "from time import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c0d9c6",
   "metadata": {},
   "source": [
    "## Data loading and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dc6dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_train = np.load(\"./ppi/adj_train.npz\")\n",
    "features = np.load(\"./ppi/feats.npy\")\n",
    "\n",
    "# The data for the (symmetric) adjacency matrix\n",
    "data = adj_train[\"data\"]\n",
    "data = data.astype(int)\n",
    "indices = adj_train[\"indices\"]\n",
    "indptr = adj_train[\"indptr\"]\n",
    "shape = adj_train[\"shape\"]\n",
    "\n",
    "# Load the labels\n",
    "with open(\"./ppi/class_map.json\", \"r\") as f:\n",
    "    class_map = json.loads(f.read())\n",
    "labels = np.empty((14755, 121))\n",
    "for key, value in class_map.items():\n",
    "    labels[int(key)] = value\n",
    "\n",
    "# # This is a less memory-efficient method to get the sparse torch adjacency matrix, that also requires SciPy\n",
    "# adj_matrix = csr_matrix((data, indices, indptr), shape=shape).toarray().astype(int)\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "# adj_matrix = torch.from_numpy(adj_matrix).to_sparse()\n",
    "\n",
    "# Change the SciPy csr format to torch format\n",
    "torch_first_indices = []\n",
    "for i in range(len(indptr)-1):\n",
    "    torch_first_indices += [i for ind in indices[indptr[i]:indptr[i+1]]]\n",
    "torch_first_indices = np.asarray(torch_first_indices)\n",
    "torch_indices = np.stack((torch_first_indices, indices))\n",
    "\n",
    "# The given shape for the ppi train set is much larger than the number of actual nodes in the set\n",
    "num_nodes = len(np.unique(torch_indices))\n",
    "shape_small = [num_nodes, num_nodes]\n",
    "\n",
    "# Create the adjacency matrix\n",
    "adj_matrix = torch.sparse_coo_tensor(indices=torch_indices, values=data, size=shape_small, dtype=torch.float)\n",
    "\n",
    "# Calculate the node degrees\n",
    "degree = [0.0 for i in range(num_nodes)]\n",
    "for i in torch_indices.T:\n",
    "    degree[i[0]] += 1\n",
    "    if i[1] == i[0]:\n",
    "        degree[i[0]] += 1\n",
    "inverse_degree = np.reciprocal(np.asarray(degree))\n",
    "# print(inverse_degree.shape)\n",
    "# print(inverse_degree)\n",
    "\n",
    "# Calculate the normalized adjacency matrix\n",
    "norm_adj_data = inverse_degree[torch_indices[0]]*data.astype(float)\n",
    "\n",
    "# norm_adj_matrix = torch.sparse_coo_tensor(indices=torch_indices, values=norm_adj_data.astype(float), size=shape_small,\n",
    "#                                           dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d830648",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c549cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_sampler(num_nodes, budget, p_nodes, p_edges, N, indices):\n",
    "    return np.unique(np.random.choice(np.arange(num_nodes), size=budget, p=p_nodes, replace=True))\n",
    "\n",
    "def edge_sampler(num_nodes, budget, p_nodes, p_edges, N, indices):\n",
    "    p_edges = p_edges/p_edges.sum()\n",
    "    edge_ids = np.unique(np.random.choice(np.arange(indices.shape[1]), size=budget, p=p_edges, replace=True))\n",
    "    nodes_s = np.unique(np.take(indices, list(edge_ids)).flatten())\n",
    "    return nodes_s\n",
    "\n",
    "def custom_sampler(num_nodes, budget, p_nodes, p_edges, N, indices):\n",
    "    nodes_s = np.unique(np.random.choice(np.arange(num_nodes), size=budget, p=p_nodes, replace=True))\n",
    "    for i in range(1, N):\n",
    "        condition = np.any(np.in1d(indices.flatten(), nodes_s).reshape(indices.shape), axis=0)\n",
    "        expansion = np.unique(indices[:,condition].flatten())\n",
    "        neighbours = np.setdiff1d(expansion, nodes_s)\n",
    "        p_neigbours = p_nodes[neighbours]\n",
    "        p_neigbours = p_neigbours/p_neigbours.sum()\n",
    "        nodes_s = np.union1d(nodes_s, np.unique(np.random.choice(neighbours, size=budget, p=p_neigbours, replace=True)))\n",
    "    return nodes_s\n",
    "\n",
    "# Note that p_nodes_sampler is used as a parameter of sampling, while p_nodes represent the true probabilities\n",
    "def sample_nodes(num_nodes, budget, p_nodes_sampler, p_nodes, p_edges, N, indices, data, features, labels, sampler):\n",
    "    # Sample\n",
    "    nodes_s = sampler(num_nodes, budget, p_nodes_sampler, p_edges, N, indices)\n",
    "    num_nodes_s = len(nodes_s)\n",
    "    \n",
    "    # Connect the sampled nodes\n",
    "    condition = np.all(np.in1d(indices.flatten(), nodes_s).reshape(indices.shape), axis=0)\n",
    "    edges_s = indices[:,condition]\n",
    "    data_s = data[condition]\n",
    "    edge_indices = np.where(condition)[0]\n",
    "    nodes_s = set(nodes_s)\n",
    "\n",
    "    # Remove unconnected nodes (not connected to other nodes or themselves)\n",
    "    len_before = len(nodes_s)\n",
    "    nodes_s = nodes_s.intersection(set(np.unique(edges_s)))\n",
    "    len_after = len(nodes_s)\n",
    "    num_nodes_s -= len_before-len_after\n",
    "    \n",
    "    # If no nodes are connected, retry sampling\n",
    "    if len(nodes_s) == 0:\n",
    "        return sample_nodes(num_nodes, budget, p_nodes, p_edges, indices, data, features)\n",
    "    \n",
    "    orig2sub = {ind : i for i, ind in enumerate(nodes_s)}\n",
    "    nodes_s_sub = {orig2sub[node] for node in nodes_s}\n",
    "    edges_s_sub = np.vectorize(orig2sub.get)(edges_s)\n",
    "\n",
    "    # Create the adjacency matrix of the sampled graph\n",
    "    adj_matrix_s = torch.sparse_coo_tensor(indices=edges_s_sub, values=data_s, size=[num_nodes_s, num_nodes_s],\n",
    "                                           dtype=torch.float)\n",
    "    \n",
    "    # Calculate the normalizing constants\n",
    "    p_nodes_s = np.take(p_nodes, list(edges_s[0]))\n",
    "    p_edges_s = np.take(p_edges, list(edge_indices))\n",
    "    alpha = p_edges_s/p_nodes_s\n",
    "    alpha_recip = 1/alpha\n",
    "    alpha_matrix = torch.sparse_coo_tensor(indices=edges_s_sub, values=alpha_recip, size=[num_nodes_s, num_nodes_s],\n",
    "                                           dtype=torch.float)\n",
    "    \n",
    "    # Select the relevant features, labels and loss normalizers\n",
    "    features_s = torch.from_numpy(features[list(nodes_s)]).to(torch.float)\n",
    "    labels_s = torch.from_numpy(labels[list(nodes_s)]).to(torch.float)\n",
    "    norm_loss = torch.from_numpy(num_nodes*np.take(p_nodes, list(nodes_s))).to(torch.float).reshape(len(nodes_s),1)\n",
    "    return adj_matrix_s * alpha_matrix, features_s, labels_s, norm_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7657d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "budget = 6000\n",
    "N = 0\n",
    "\n",
    "# Calculate the sampling probablities\n",
    "p_nodes = [0.0 for i in range(shape_small[0])]\n",
    "for i, ind in enumerate(torch_indices[1]):\n",
    "    p_nodes[ind] += norm_adj_data[i]**2\n",
    "p_nodes = np.asarray(p_nodes)\n",
    "p_nodes = p_nodes/p_nodes.sum()\n",
    "\n",
    "# Get the edge probabilities\n",
    "# For the node sampler, the probability of an edge being sampled, is just to probability of both it's nodes being sampled\n",
    "# Note that for an edge connecting a node to itself, the probability of sampling it is just the probability of sampling the node\n",
    "self_loops = np.where(np.all(torch_indices == torch_indices[0,:], axis = 0)==True)\n",
    "p_edges = np.take(p_nodes, torch_indices)\n",
    "np.put(p_edges[1], self_loops, 1)\n",
    "p_edges = p_edges.prod(0)\n",
    "\n",
    "# p_edge_matrix = torch.sparse_coo_tensor(indices=torch_indices, values=p_edges, size=shape_small, dtype=torch.float64)\n",
    "nodewise_sampler = partial(sample_nodes, num_nodes, budget, p_nodes, p_nodes, p_edges, N, torch_indices, norm_adj_data,\n",
    "                           features, labels, node_sampler)\n",
    "# nodewise_sampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05eec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "budget = 2000\n",
    "N = 3\n",
    "num_samples = 1000\n",
    "\n",
    "# Sample using the node-wise probabilities to start, sample subgraphs and count node-occurances\n",
    "# Note that p_nodes could have been re-used from the node-wise calculations\n",
    "p_nodes = [0.0 for i in range(shape_small[0])]\n",
    "for i, ind in enumerate(torch_indices[1]):\n",
    "    p_nodes[ind] += norm_adj_data[i]**2\n",
    "p_nodes = np.asarray(p_nodes)\n",
    "p_nodes = p_nodes/p_nodes.sum()\n",
    "\n",
    "# To make the algorithm more efficient, these samples could be reused for training\n",
    "count_nodes = np.asarray([0.0 for i in range(shape_small[0])])\n",
    "for i in range(num_samples):\n",
    "    nodes_s = custom_sampler(num_nodes, budget, p_nodes, p_edges, N, torch_indices)\n",
    "    for node in nodes_s:\n",
    "        count_nodes[node] += 1\n",
    "# Add a small amount to the counts if needed, to prevent division by zero errors down the line\n",
    "if 0 in count_nodes:\n",
    "    count_nodes += 0.1\n",
    "p_nodes_approx = count_nodes/count_nodes.sum()\n",
    "\n",
    "# Get the edge probabilities\n",
    "self_loops = np.where(np.all(torch_indices == torch_indices[0,:], axis = 0)==True)\n",
    "p_edges_approx = np.take(p_nodes, torch_indices)\n",
    "np.put(p_edges_approx[1], self_loops, 1)\n",
    "p_edges_approx = p_edges_approx.prod(0)\n",
    "\n",
    "ladies_sampler = partial(sample_nodes, num_nodes, budget, p_nodes, p_nodes_approx, p_edges_approx, N, torch_indices,\n",
    "                         norm_adj_data, features, labels, custom_sampler)\n",
    "# ladies_sampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39836363",
   "metadata": {},
   "outputs": [],
   "source": [
    "budget = 4000\n",
    "N = 0\n",
    "\n",
    "# Calculate the edge probabilities based on the calculated degrees\n",
    "p_edges = inverse_degree[torch_indices[0]] + inverse_degree[torch_indices[1]]\n",
    "p_edges = p_edges/p_edges.sum()\n",
    "\n",
    "# Calcualte the node probabilities using the edge probabilities\n",
    "not_p_edges = 1 - p_edges\n",
    "not_p_nodes = [1.0 for i in range(shape_small[0])]\n",
    "for i, edge in enumerate(torch_indices.transpose()):\n",
    "    not_p_nodes[edge[0]] *= not_p_edges[i]\n",
    "not_p_nodes = np.asarray(not_p_nodes)\n",
    "p_nodes = 1 - not_p_nodes\n",
    "\n",
    "edgewise_sampler = partial(sample_nodes, num_nodes, budget, p_nodes, p_nodes, p_edges, N, torch_indices, norm_adj_data,\n",
    "                           features, labels, edge_sampler)\n",
    "# edgewise_sampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30777e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myGraphSAINT(nn.Module):\n",
    "    def __init__(self, hidden_sizes, lr, p_dropout, sampler):\n",
    "        super(myGraphSAINT, self).__init__()\n",
    "        # The weights are initialized according to the default pytorch initialization for linear layers\n",
    "        self.weights = torch.nn.ParameterList(\n",
    "            nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((hidden_sizes[i], hidden_sizes[i+1])), gain=1/np.sqrt(6.0)))\n",
    "            for i in range(len(hidden_sizes)-1)\n",
    "        )\n",
    "        self.p_dropout = p_dropout\n",
    "        self.sampler = sampler\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "    \n",
    "    def forward(self):\n",
    "        A, x, labels_s, norm_loss = self.sampleGraph()\n",
    "        for W in self.weights:\n",
    "            x = nn.functional.relu(A @ x @ W)\n",
    "            x = torch.nn.Dropout(p=self.p_dropout)(x)\n",
    "        x = nn.Sigmoid()(x)\n",
    "        return x, labels_s, norm_loss\n",
    "    \n",
    "    def sampleGraph(self):\n",
    "        return self.sampler()\n",
    "    \n",
    "    def train_step(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        preds, labels_s, norm_loss = self()\n",
    "        loss = torch.nn.BCEWithLogitsLoss(weight=norm_loss,reduction='sum')(preds, labels_s)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()/preds.size(0)\n",
    "    \n",
    "    def fit(self, num_iterations):\n",
    "        self.train()\n",
    "        losses = []\n",
    "        for i in range(num_iterations):\n",
    "            losses.append(self.train_step())\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847b2b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_its = 1000\n",
    "hidden_size = 128\n",
    "\n",
    "nodewise_model = myGraphSAINT([50, hidden_size, 121], 0.01, 0.0, nodewise_sampler)\n",
    "edgewise_model = myGraphSAINT([50, hidden_size, 121], 0.01, 0.1, edgewise_sampler)\n",
    "ladies_model = myGraphSAINT([50, hidden_size, 121], 0.01, 0.0, ladies_sampler)\n",
    "t1 = time()\n",
    "losses_nodewise = nodewise_model.fit(n_its)\n",
    "t2 = time()\n",
    "print(\"Node-Wise Training Time:\", t2-t1)\n",
    "losses_edgewise = edgewise_model.fit(n_its)\n",
    "t3 = time()\n",
    "print(\"Edge-Wise Training Time:\", t3-t2)\n",
    "losses_ladies = ladies_model.fit(n_its)\n",
    "t4 = time()\n",
    "print(\"Ladies Training Time:\", t4-t3)\n",
    "\n",
    "print()\n",
    "print(np.asarray(losses_nodewise))\n",
    "print()\n",
    "print(np.asarray(losses_edgewise))\n",
    "print()\n",
    "print(np.asarray(losses_ladies))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
